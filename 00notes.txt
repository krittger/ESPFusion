2020-04-27

Timings for 4 test days:

Job ID	     Date	Time (wall clock)
4602811_1850 20050123   04:51:00
4602811_2530 20061204   04:54:00
4602811_3014 20080401   04:40:00
4602811_4080 20110303   04:44:00

All results (downscaled/regression/prob.hundred/prob.btwn .tif
images) are identical to the ones Will generated:

/pl/active/SierraBighorn/downscaledv3/downscaled/3e+05.test/

2020-05-26

Once we have produced daily SSN.*.snow_fraction.*tif files using
the SnowTodayV00 mosaics as input, we need to re-run the
preprocessing Will and Mitch set up, to produce input lookup
files by year.

This will facilitate sbatch scripts as job arrays for
n=1,ndaysinyear.

Consider option to separate the parts of the setup file that are
static and those that are related to the dates/files we are
processing.

What's currently in it?

See notes in

https://nsidc.org/jira/browse/SIER-109

2020-06-24

Have done the planned split described in SIER-109, and changed
the downscaling interface to take year and dayOfYear to process.

Job ID	     Date	Time (wall clock)
5364235_23   20050123   05:10:00
5372524_62   20110303   05:00:00
5372524_92   20080401   05:30:00
5372524_338  20061204   05:15:00

To check differences with Will's output, use scripts/regression_compare.sh.

These runs are also identical to Will's output.

2020-06-25

Added switch to read old MODIS data and resample, but to read new
MODIS data at high Res.  Retested regression days at old
resolution, output are identical.

Job ID	     Date	Time (wall clock)
5394640_23   20050123   04:50:00
5394640_62   20110303   05:20:00
5394640_92   20080401   05:20:00
5394640_338  20061204   05:15:00

2020-07-21

Added -f switch to not clobber prior output, unless -f switch is
TRUE
Retested regression days at old resolution, output are identical.

Job ID	     Date	Time (wall clock)
5554205_23   20050123
5554205_62   20110303
5554205_92   20080401
5554205_338  20061204

Output are identical.

2020-08-10

Running regression on modis v3 data with Shalini's latest model
(v4).

Job ID	     Date
5630511_23   20050123
5630511_62   20110303
5630511_92   20080401
5630511_338  20061204

Produced output, but wrote "v3" filenames.

Fixed this, but then realized Shalini didn't produce the same
dates that Will did.  Fixed the 4 regression dates and started
over:

Job ID	     Date     Start            Stop             Duration
5631565_22   20050122 2020-08-10 17:16 2020-08-10 21:25 4h 09m
5631565_345  20061211 2020-08-10 17:24 2020-08-10 21:34 4h 10m
5631565_116  20040425 2020-08-10 17:24 2020-08-10 21:32 4h 08m
5631565_71   20110312 2020-08-10 17:17 2020-08-10 21:23 4h 06m

2020-08-12

My results don't match Shalini's results. Realized my classifier
run on the model is set to use num.trees=50, but examined the
classifier model file, which has this value set to 100.  Will
says this will definitely affect output values.  Trying a new run
with no value set in the predict call (which is how Shalini is
running it). This was the value that Will played with when he
reduced the memory requirements to run on summit.  I think
Shalini is running on a blanca node so she doesn't have to worry.

Started a new regression job, used top to watch memory, it is
definitely bumping up against 100GB memory capacity, but doesn't
appear to be crashing (is R smart enough to start swapping?).

Job ID	     Date     Start            Stop             Duration
5646881_22   20050122 2020-08-13 03:16 2020-08-13 08:26 5h 10m
5646881_345  20061211 2020-08-13 03:16 2020-08-13 08:07 4h 50m
5646881_116  20040425 2020-08-13 03:16 2020-08-13 08:49 5h 33m
564688a_71   20110312 2020-08-13 03:16 2020-08-13 08:23 5h 7m

So running with 100 trees definitely takes longer and bumps
memory limits.

Outputs don't match Shalini's outputs.

Asked her to set up a run where she sets num.trees=50. The other
thing that's different is that I'm running on Summit and she's on
Blanca.  Maybe there are precision issues?

Added year subdirectory to output location trees.  I'm just going
to start running with nTrees=50 on summit.  We need to start
generating data.

To get duration of daily jobs, use:

sacct -j 5656748 --format=jobid,jobname,user,elapsed,state | grep two > run.twostep.downscaling.summit.durations.2019.5656748.out

Started with numtrees=50.

Year	Queue  JobID   Started          Finished         Duration  Status
2020
	       doys should be through 12 Jun (1-164)
2019 D  Summit 5656748 2020-08-16 16:41 2020-08-17 15:02 22h 20m   no errors/warnings
     	       Most jobs < 5h, a couple took
	       7.5 hours.
2018 D	Summit 5664848 2020-08-17 14:55 2020-08-19 13:04 47h 0m    no errors/warnings
	       	       		  		   	     	   Most took < 5 h
2017 D  Summit 5681018 2020-08-19 13:25 2020-08-21 14:52 49h 20m   no errors/warnings
     	       Most jobs 4-5h, one took 7:50. no errors/warnings.
2016 D  blanca-preemptable
	       9694236 2020-08-20 16:13 2020-08-21 19:44 27h 30m   Mixed, MaxSignal[9]
	       most jobs took 3-4 hours, no warnings or
	       errors in output, complete set of 366
	       files is there.  Maybe the mixed
	       signal was from the original error
	       message?
2015 D   Summit 5701271 2020-08-22 19:43 2020-08-24 06:41 35h       no errors/warnings
	       Most jobs took 4-5 hours, one took 7:50!
2014 D  blanca-preemptable
	       9696148 2020-08-22 19:46 2020-08-23 17:52  22h       no errors/warnings	       
	       Most jobs took 3-4.5 hours, 6 took > 5h
2013 D  blanca-preemptable (added --exclude command, error still happened)
	       9697419 2020-08-23 17:54 2020-08-24 03:43   9h       no errors/warnings
	       Most jobs took 3-4.5 hours, 6 took > 5h	       
*********************************
New set of switches on blanca-preemptable, from Andy.
2012    Summit 5716943 2020-08-24 13:38
2011
2010
2009
2008
2007
2006
2005
2004
2003
2002
2001
2000


